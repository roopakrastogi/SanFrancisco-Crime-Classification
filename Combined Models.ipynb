{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "import pickle\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import neighbors\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors.classification import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as mt\n",
    "import matplotlib.image as mpimg\n",
    "from pandas.plotting import scatter_matrix\n",
    "import string\n",
    "import re\n",
    "from sklearn.preprocessing import Imputer,LabelEncoder,OneHotEncoder\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFOCrimeClass1(object):\n",
    "    \n",
    "    def __init__(self, trainFile, testFile):\n",
    "        self.trainFile = trainFile\n",
    "        self.testFile = testFile\n",
    "        \n",
    "        self.lr = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "        self.rf = RandomForestClassifier(n_estimators = 500, max_depth=15,oob_score =True)\n",
    "        self.lgbm = lightgbm.LGBMClassifier(objective = 'multiclass',n_estimators = 200,max_depth = 12)\n",
    "        self.xgb_model = None\n",
    "        \n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_val = None\n",
    "        self.y_val = None\n",
    "        \n",
    "        self.prob_val_lr = None\n",
    "        self.prob_val_rf = None\n",
    "        self.prob_val_lgbm = None\n",
    "        self.prob_val_xgb = None\n",
    "        \n",
    "        self.prob_test_lr = None\n",
    "        self.prob_test_rf = None\n",
    "        self.prob_test_lgbm = None\n",
    "        self.prob_test_xgb = None\n",
    "        \n",
    "        self.test_data = None\n",
    "        self.result =  None\n",
    "        self.testset_ID = None\n",
    "        \n",
    "    def trainingData(self):\n",
    "        tr_dataset = pd.read_csv(self.trainFile,parse_dates=['Dates'])\n",
    "        \n",
    "        #Label Encoding of Crime Category\n",
    "        cat_le            =  preprocessing.LabelEncoder()\n",
    "        crimecat          =  cat_le.fit_transform(tr_dataset.Category)\n",
    "        \n",
    "        #Datetime parser\n",
    "        tr_hour  = tr_dataset.Dates.dt.hour\n",
    "        tr_year  = tr_dataset.Dates.dt.year\n",
    "\n",
    "        #Applying PCA on X and Y cordinate\n",
    "        xy_scaler_tr = preprocessing.StandardScaler()\n",
    "        xy_scaler_tr.fit(tr_dataset[[\"X\",\"Y\"]])\n",
    "        tr_dataset[[\"X\",\"Y\"]] = xy_scaler_tr.transform(tr_dataset[[\"X\",\"Y\"]])\n",
    "        pca_tr = PCA(2)\n",
    "        tr_dataset.loc[:,[\"X\",\"Y\"]]=pca_tr.fit_transform(pd.DataFrame(tr_dataset.loc[:,[\"X\",\"Y\"]]))\n",
    "        tr_dataset[\"rot30_X\"]  = (1.732/2)* tr_dataset[\"X\"] + (1./2)* tr_dataset[\"Y\"] \n",
    "        tr_dataset[\"rot30_Y\"]  = (1.732/2)* tr_dataset[\"Y\"] - (1./2)* tr_dataset[\"X\"]\n",
    "        tr_dataset[\"rot45_X\"]  = .707* tr_dataset[\"Y\"] + .707* tr_dataset[\"X\"] \n",
    "        tr_dataset[\"rot45_Y\"]  = .707* tr_dataset[\"Y\"] - .707* tr_dataset[\"X\"]\n",
    "        tr_dataset[\"rot60_X\"]  = (1./2)* tr_dataset[\"X\"] + (1.732/2)* tr_dataset[\"Y\"] \n",
    "        tr_dataset[\"rot60_Y\"]  = (1./2)* tr_dataset[\"Y\"] - (1.732/2)* tr_dataset[\"X\"]\n",
    "        tr_dataset[\"radial_r\"] = np.sqrt( np.power(tr_dataset[\"Y\"],2) + np.power(tr_dataset[\"X\"],2) )\n",
    "        \n",
    "        #Extracting whther crime happening at juntion or in block\n",
    "        tr_add           = tr_dataset[\"Address\"]\n",
    "        tr_add_ser       = tr_add.str.contains('.?of.?')\n",
    "        #Onehot Encoding categorical features\n",
    "        tr_hour_ser      = pd.get_dummies(tr_hour,prefix = \"H\")\n",
    "        tr_year_ser      = pd.get_dummies(tr_year)\n",
    "        tr_days_ser      = pd.get_dummies(tr_dataset.DayOfWeek)\n",
    "        tr_pdistrict_ser = pd.get_dummies(tr_dataset.PdDistrict)\n",
    "        \n",
    "        trainingset      = pd.concat([tr_hour_ser,\n",
    "                                      tr_year_ser,\n",
    "                                      tr_days_ser,\n",
    "                                      tr_pdistrict_ser,\n",
    "                                      tr_add_ser,\n",
    "                                      tr_dataset.X,tr_dataset.Y,\n",
    "                                      tr_dataset.rot30_X,tr_dataset.rot30_Y,\n",
    "                                      tr_dataset.rot45_X,tr_dataset.rot45_Y,\n",
    "                                      tr_dataset.rot60_X,tr_dataset.rot60_Y,\n",
    "                                      tr_dataset.radial_r],axis=1)\n",
    "        #split data into train and validation\n",
    "        seed = 7\n",
    "        test_size = 0.2\n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(trainingset, crimecat, \n",
    "                                                                              test_size=test_size, \n",
    "                                                                              random_state=seed)\n",
    "   \n",
    "    def testingData(self):\n",
    "        te_dataset = pd.read_csv(self.testFile,parse_dates=['Dates'])\n",
    "     \n",
    "        #Datetime parsering from test data\n",
    "        te_hour  = te_dataset.Dates.dt.hour\n",
    "        te_year  = te_dataset.Dates.dt.year\n",
    "        \n",
    "        #Applying PCA on X and Y cordinates from test\n",
    "        xy_scaler_te = preprocessing.StandardScaler()\n",
    "        xy_scaler_te.fit(te_dataset[[\"X\",\"Y\"]])\n",
    "        te_dataset[[\"X\",\"Y\"]] = xy_scaler_te.transform(te_dataset[[\"X\",\"Y\"]])\n",
    "        pca_te =PCA(2)\n",
    "        te_dataset.loc[:,[\"X\",\"Y\"]] = pca_te.fit_transform(pd.DataFrame(te_dataset.loc[:,[\"X\",\"Y\"]]))\n",
    "        te_dataset[\"rot30_X\"]  = (1.732/2)* te_dataset[\"X\"] + (1./2)* te_dataset[\"Y\"] \n",
    "        te_dataset[\"rot30_Y\"]  = (1.732/2)* te_dataset[\"Y\"] - (1./2)* te_dataset[\"X\"]\n",
    "        te_dataset[\"rot45_X\"]  = .707* te_dataset[\"Y\"] + .707* te_dataset[\"X\"] \n",
    "        te_dataset[\"rot45_Y\"]  = .707* te_dataset[\"Y\"] - .707* te_dataset[\"X\"]\n",
    "        te_dataset[\"rot60_X\"]  = (1./2)* te_dataset[\"X\"] + (1.732/2)* te_dataset[\"Y\"] \n",
    "        te_dataset[\"rot60_Y\"]  = (1./2)* te_dataset[\"Y\"] - (1.732/2)* te_dataset[\"X\"]\n",
    "        te_dataset[\"radial_r\"] = np.sqrt( np.power(te_dataset[\"Y\"],2) + np.power(te_dataset[\"X\"],2) )\n",
    "        #Extracting whther crime happening at juntion or in block\n",
    "        te_add           = te_dataset[\"Address\"]\n",
    "        te_add_ser       = te_add.str.contains('.?of.?')\n",
    "        #Onehot Encoding categorical features\n",
    "        te_hour_ser      = pd.get_dummies(te_hour,prefix = \"H\")\n",
    "        te_year_ser      = pd.get_dummies(te_year)\n",
    "        te_days_ser      = pd.get_dummies(te_dataset.DayOfWeek)\n",
    "        te_pdistrict_ser = pd.get_dummies(te_dataset.PdDistrict)\n",
    "        \n",
    "        testset          = pd.concat([te_hour_ser,\n",
    "                                      te_year_ser,\n",
    "                                      te_days_ser,\n",
    "                                      te_pdistrict_ser,\n",
    "                                      te_add_ser,\n",
    "                                      te_dataset.X,te_dataset.Y,\n",
    "                                      te_dataset.rot30_X,te_dataset.rot30_Y,\n",
    "                                      te_dataset.rot45_X,te_dataset.rot45_Y,\n",
    "                                      te_dataset.rot60_X,te_dataset.rot60_Y,\n",
    "                                      te_dataset.radial_r],axis=1)\n",
    "        \n",
    "        self.test_data = testset\n",
    "        self.testset_ID = te_dataset['Id']\n",
    "        \n",
    "    def data(self):\n",
    "        self.trainingData()\n",
    "        self.testingData()\n",
    "#===========Logistic Regression================================================#\n",
    "    def trainLogisticRegression(self):\n",
    "        self.lr.fit(self.X_train,self.y_train)\n",
    "        self.valLogisticRegression()\n",
    "\n",
    "    def valLogisticRegression(self):\n",
    "        self.prob_val_lr =  self.lr.predict_proba(self.X_val) \n",
    "        print(\"Logistic Regression logloss \" + str(log_loss(self.y_val,self.prob_val_lr)))\n",
    "    \n",
    "    def testLogisticRegression(self):\n",
    "        self.prob_test_lr = self.lr.predict(self.test_data)\n",
    "#===========Random Forest======================================================#        \n",
    "    def trainRandomForest(self):\n",
    "        self.rf.fit(self.X_train,self.y_train)\n",
    "        self.valRandomForest()\n",
    "\n",
    "    def valRandomForest(self):\n",
    "        self.prob_val_rf = self.rf.predict_proba(self.X_val)\n",
    "        print(\"Random Forest logloss \" + str(log_loss(self.y_val,self.prob_val_rf)))\n",
    "    \n",
    "    def testRandomForest(self):\n",
    "        self.prob_test_rf = self.rf.predict(self.test_data)\n",
    "#===========LightGBM============================================================#        \n",
    "    def trainLGBM(self):\n",
    "        self.lgbm.fit(self.X_train,self.y_train)\n",
    "        self.valLGBM()\n",
    "\n",
    "    def valLGBM(self):\n",
    "        self.prob_val_lgbm = self.lgbm.predict_proba(self.X_val)\n",
    "        print(\"LightGBM logloss \" + str(log_loss(self.y_val,self.prob_val_lgbm)))\n",
    "    \n",
    "    def testLGBM(self):\n",
    "        self.prob_test_rf = self.lgbm.predict(self.test_data)\n",
    "#===========XGBoost=============================================================#\n",
    "    def trainXGB(self):\n",
    "        #setting parameter for xgboost\n",
    "        param = {}\n",
    "        param['booster']               = 'gbtree'\n",
    "        param['objective']             = 'multi:softprob'\n",
    "        param['num_class']             = 36\n",
    "        param['eval_metric']           = 'mlogloss'\n",
    "        # param['scale_pos_weight']    = 1.0\n",
    "        param['eta']                   = .3\n",
    "        # param['max_depth']           = 6\n",
    "        param['bst:colsample_bytree']  = 0.6\n",
    "        # param['gamma']               = 0.5\n",
    "        # param['min_child_weight']    = .5\n",
    "        param['max_delta_step']        = 1\n",
    "        param['silent']                = 1\n",
    "        param['early_stopping_rounds'] = 30\n",
    "        param['nthread']               = 4\n",
    "        param['verbose_eval']          = 1\n",
    "        num_round = 200\n",
    "        plst = list(param.items())\n",
    "        \n",
    "        #training model\n",
    "        xgb_training   = xgb.DMatrix(self.X_train, label = self.y_train)\n",
    "        xgb_valset     = xgb.DMatrix(self.X_val,label = self.y_val)\n",
    "        watchlist      = [(xgb_training ,'train'),(xgb_valset,'validation')]\n",
    "        self.xgb_model = xgb.train(plst,xgb_training, num_round,watchlist)\n",
    "        self.valXGB()\n",
    "        \n",
    "    def valXGB(self):\n",
    "        losses = []\n",
    "        xgb_valset        = xgb.DMatrix(self.X_val,label = self.y_val)\n",
    "        self.prob_val_xgb = self.xgb_model.predict(xgb_valset)\n",
    "        ll = log_loss(self.y_val, self.prob_val_xgb)\n",
    "        losses.append(ll)\n",
    "        print(\"XGBoost logloss \"+ str(np.mean(losses)))\n",
    "        \n",
    "    def testXGB(self):\n",
    "        xgb_testset        = xgb.DMatrix(self.test_data)\n",
    "        self.prob_test_xgb = xgb_model.predict(xgb_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFOCrimeClass2(object):\n",
    "    \n",
    "    def __init__(self, trainFile, testFile):\n",
    "        self.trainFile = trainFile\n",
    "        self.testFile = testFile\n",
    "        \n",
    "        self.knn = neighbors.KNeighborsRegressor(n_neighbors=1200,n_jobs=500,weights='distance')\n",
    "        self.kmeans = None\n",
    "        self.xgb_model = None\n",
    "        self.gnb = GaussianNB()\n",
    "        self.bnb = BernoulliNB()\n",
    "        self.gnbmix = GaussianNB()\n",
    "        self.bnbmix = BernoulliNB()\n",
    "        \n",
    "        self.features1 = None\n",
    "        self.features2 = None\n",
    "        self.training = None\n",
    "        self.validation = None\n",
    "        self.valcrime = None\n",
    "        self.traincrime = None\n",
    "        \n",
    "        self.hour = None\n",
    "        self.dis_le = None\n",
    "        self.crimelist = None\n",
    "        \n",
    "        self.prob_val_knn = None\n",
    "        self.prob_val_gnb = None\n",
    "        self.prob_val_bnb = None\n",
    "        self.prob_val_bgnbmix = None\n",
    "        \n",
    "        self.prob_test_knn = None\n",
    "        self.prob_test_lgbm = None\n",
    "        self.prob_test_gnb = None\n",
    "        self.prob_test_bnb = None\n",
    "        self.prob_test_bgnbmix = None\n",
    "        \n",
    "        self.test_data_1 = None\n",
    "        self.test_data = None\n",
    "        self.K_test = None\n",
    "        self.result =  None\n",
    "        \n",
    "    def trainingData(self):\n",
    "        tr_dataset = pd.read_csv(self.trainFile,parse_dates=['Dates'])\n",
    "        tr_data = tr_dataset\n",
    "        #Label Encoding of Crime Category\n",
    "        cat_le            =  preprocessing.LabelEncoder()\n",
    "        crimecat          =  cat_le.fit_transform(tr_dataset.Category)\n",
    "        self.crimelist    =  pd.get_dummies(tr_dataset.Category)\n",
    "        self.dis_le       =  preprocessing.LabelEncoder()\n",
    "        pddis             =  self.dis_le.fit_transform(tr_dataset.PdDistrict)\n",
    "        tr_dataset[\"crime\"] = crimecat\n",
    "        #Datetime parser\n",
    "        tr_hour  = tr_dataset.Dates.dt.hour\n",
    "        tr_year  = tr_dataset.Dates.dt.year\n",
    "        tr_minu = tr_dataset.Dates.dt.minute\n",
    "        \n",
    "        #Generating K-Mean\n",
    "        \n",
    "        kmeans               = KMeans(n_clusters=120,max_iter=300)\n",
    "        df     = pd.DataFrame(pd.concat([pd.DataFrame(tr_dataset.X),pd.DataFrame(tr_dataset.Y)],axis=1))\n",
    "        kmeans.fit(df)\n",
    "        labels               = kmeans.predict(df)\n",
    "        centroids            = kmeans.cluster_centers_\n",
    "        tr_dataset[\"Klabels\"]=labels\n",
    "        self.kmeans = kmeans\n",
    "        \n",
    "        #Applying PCA on X and Y cordinate\n",
    "        X = tr_dataset.X\n",
    "        Y = tr_dataset.Y\n",
    "        \n",
    "        \n",
    "        #Generating SVD of Address to use them as numerical values\n",
    "        vectorizer = HashingVectorizer(n_features=5)\n",
    "        vector = vectorizer.transform(tr_dataset.Address)\n",
    "        svd=TruncatedSVD(n_components=2, random_state=42)\n",
    "        tr_dataset[\"AddSVD\"]=pd.DataFrame(svd.fit_transform(vector)).loc[:,1]\n",
    "        \n",
    "        #Onehot Encoding categorical features\n",
    "        tr_hour_ser      = pd.get_dummies(tr_hour,prefix = \"H\")\n",
    "        tr_days_ser      = pd.get_dummies(tr_dataset.DayOfWeek)\n",
    "        tr_pdistrict_ser = pd.get_dummies(tr_dataset.PdDistrict)\n",
    "        tr_min_ser       = pd.get_dummies(tr_minu)\n",
    "        self.hour = tr_hour_ser\n",
    "        # training set created for Naive Bayes and KNN \n",
    "        train_data       = pd.concat([tr_hour_ser,\n",
    "                                      tr_days_ser,\n",
    "                                      tr_pdistrict_ser,\n",
    "                                      tr_dataset.AddSVD,\n",
    "                                      tr_dataset.Klabels,\n",
    "                                      tr_min_ser,\n",
    "                                      X,Y,tr_dataset.crime],axis=1)\n",
    "    \n",
    "        tr_data[\"Hour\"]       = tr_hour\n",
    "        tr_data[\"Minutes\"]    = tr_minu\n",
    "        tr_data[\"X\"]          = X\n",
    "        tr_data[\"Y\"]          = Y\n",
    "        tr_data[\"PdDistrict\"] = pddis\n",
    "\n",
    "        tr_data = pd.concat([tr_data,self.crimelist],axis =1)\n",
    "        #split data into train and validation\n",
    "        seed = 7\n",
    "        test_size = 0.2\n",
    "        self.training, self.validation = train_test_split(train_data, test_size=test_size,random_state=seed)\n",
    "        self.K_train,self.K_val = train_test_split(tr_data, test_size= test_size, random_state=seed)\n",
    "   \n",
    "    def testingData(self):\n",
    "        te_dataset = pd.read_csv(self.testFile,parse_dates=['Dates'])\n",
    "        test_KNN = te_dataset\n",
    "        #Datetime parsering from test data\n",
    "        te_hour  = te_dataset.Dates.dt.hour\n",
    "        te_year  = te_dataset.Dates.dt.year\n",
    "        te_min   = te_dataset.Dates.dt.minute\n",
    "        pddis    = self.dis_le.transform(te_dataset.PdDistrict)\n",
    "        # Computing K-Means on X and Y cordinates\n",
    "        df     = pd.DataFrame(pd.concat([pd.DataFrame(te_dataset.X),pd.DataFrame(te_dataset.Y)],axis=1))\n",
    "        labels               = self.kmeans.predict(df)\n",
    "        centroids            = self.kmeans.cluster_centers_\n",
    "        te_dataset[\"Klabels\"]=labels\n",
    "        \n",
    "        #Applying PCA on X and Y cordinates from test\n",
    "        X = te_dataset.X\n",
    "        Y = te_dataset.Y\n",
    "                \n",
    "        #Generating SVD of Address to use it as numerical values\n",
    "        vectorizer = HashingVectorizer(n_features=10)\n",
    "        vector = vectorizer.transform(te_dataset.Address)\n",
    "        svd=TruncatedSVD(n_components=2, random_state=42)\n",
    "        te_dataset[\"AddSVD\"]=pd.DataFrame(svd.fit_transform(vector)).loc[:,1]\n",
    "        \n",
    "        #Onehot Encoding categorical features\n",
    "        te_hour_ser      = pd.get_dummies(te_hour,prefix = \"H\")\n",
    "        te_days_ser      = pd.get_dummies(te_dataset.DayOfWeek)\n",
    "        te_pdistrict_ser = pd.get_dummies(te_dataset.PdDistrict)\n",
    "        te_min_ser       = pd.get_dummies(te_dataset.Dates.dt.minute)\n",
    "        testset_ID = te_dataset['Id']\n",
    "            \n",
    "        # training set created for Naive Bayes and KNN \n",
    "        test_data_1       = pd.concat([te_hour_ser,\n",
    "                                      te_days_ser,\n",
    "                                      te_pdistrict_ser,\n",
    "                                      te_dataset.AddSVD,\n",
    "                                      te_dataset.Klabels,\n",
    "                                      te_min_ser,\n",
    "                                      X,Y],axis=1)\n",
    "        self.test_data_1=test_data_1\n",
    "        test_KNN[\"Hour\"]       = te_hour\n",
    "        test_KNN[\"Minutes\"]    = te_min\n",
    "        test_KNN[\"PdDistrict\"] = pddis    \n",
    "        self.K_test = test_KNN\n",
    "        \n",
    "    def data(self):\n",
    "        self.trainingData()\n",
    "        self.testingData()\n",
    "#===========KNN========================================================#\n",
    "    def trainKNN(self):\n",
    "        features = [\"Hour\",\"Minutes\",\"X\",\"Y\",\"PdDistrict\"]\n",
    "        self.knn.fit(self.K_train[features],self.K_train[list(pd.DataFrame(self.crimelist).columns)])\n",
    "        self.valKNN()\n",
    "\n",
    "    def valKNN(self):\n",
    "        features = [\"Hour\",\"Minutes\",\"X\",\"Y\",\"PdDistrict\"]\n",
    "        self.prob_val_knn =  self.knn.predict(self.K_val[features]) \n",
    "        print(\"KNN logloss \" + str(log_loss(self.K_val[list(pd.DataFrame(self.crimelist).columns)],self.prob_val_knn)))\n",
    "    \n",
    "    def testKNN(self):\n",
    "        features = [\"Hour\",\"Minutes\",\"X\",\"Y\",\"PdDistrict\"]\n",
    "        self.prob_test_knn = self.knn.predict(self.K_test[features])\n",
    "#===========Naive Bayes================================================#\n",
    "    def trainNaiveBayes(self):\n",
    "        self.features1 = ['Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday','Wednesday', 'BAYVIEW', \n",
    "                     'CENTRAL', 'INGLESIDE', 'MISSION','NORTHERN', 'PARK', 'RICHMOND','SOUTHERN', 'TARAVAL',\n",
    "                     'TENDERLOIN'] + [x for x in range(0, 59)]+ [x for x in (set(pd.DataFrame(self.hour).columns))]\n",
    "        \n",
    "        self.features2 = ['AddSVD','Klabels','X','Y']\n",
    "        \n",
    "        # training on GaussionNaiveBayes\n",
    "        self.gnb.fit(self.training[self.features1+self.features2],self.training[\"crime\"])\n",
    "        # training on BernoulliNaiveBayes\n",
    "        self.bnb.fit(self.training[self.features1+self.features2],self.training[\"crime\"])\n",
    "        # training on mixture of Bernoulli and Gaussian Naive Bayes\n",
    "        self.bnbmix.fit(self.training[self.features1],self.training[\"crime\"])\n",
    "        self.gnbmix.fit(self.training[self.features2],self.training[\"crime\"])\n",
    "        self.valNaiveBayes()\n",
    "\n",
    "    def valNaiveBayes(self):\n",
    "        self.prob_val_gnb =  self.gnb.predict_proba(self.validation[self.features1+self.features2]) \n",
    "        print(\"Gaussian Naive Bayes logloss \" + str(log_loss(self.validation[\"crime\"],self.prob_val_gnb)))\n",
    "#      def valBNaiveBayes(self):\n",
    "        self.prob_val_bnb =  self.bnb.predict_proba(self.validation[self.features1+self.features2]) \n",
    "        print(\"Bernoulli Naive Bayes logloss \" + str(log_loss(self.validation[\"crime\"],self.prob_val_bnb)))\n",
    "#      def valMixNaiveBayes(self):\n",
    "        pos_prior = self.gnbmix.class_prior_\n",
    "        predictedBNB = np.array(self.bnbmix.predict_proba(self.validation[self.features1]))\n",
    "        predictedGNB = np.array(self.gnbmix.predict_proba(self.validation[self.features2]))\n",
    "        self.prob_val_bgnbmix = (predictedBNB)* (predictedGNB)\n",
    "        print(\"Bernoulli and Gaussian mix Naive Bayes logloss \" + str(log_loss(self.validation[\"crime\"],self.prob_val_bgnbmix)))\n",
    "        \n",
    "    def testNaiveBayes(self):\n",
    "        self.prob_test_gnb = self.gnb.predict(self.test_data_1[self.features1+self.features2])\n",
    "        self.prob_test_bnb = self.bnb.predict(self.test_data_1[self.features1+self.features2])\n",
    "        predictedBNB = np.array(self.bnbmix.predict_proba(self.test_data_1[self.features1]))\n",
    "        predictedGNB = np.array(self.gnbmix.predict_proba(self.test_data_1[self.features2]))\n",
    "        self.prob_test_bgnbmix = (predictedBNB)* (predictedGNB)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFOVisualisation(object):\n",
    "    \n",
    "    def __init__(self, trainFile):\n",
    "        self.train = pd.read_csv(trainFile,parse_dates=['Dates'])\n",
    "        self.train[\"Year\"] = self.train.Dates.dt.year\n",
    "        self.train[\"Month\"] = self.train.Dates.dt.month\n",
    "        self.train[\"Date\"] = self.train.Dates.dt.date\n",
    "        self.train[\"Hour\"] = self.train.Dates.dt.hour\n",
    "        self.train.drop(columns=[\"Resolution\"],axis=1,inplace=True)\n",
    "        self.dataplot = self.train.copy()\n",
    "\n",
    "    def categoryDistribution(self):\n",
    "        f1 = self.train.groupby(\"Category\")[\"Category\"].count().sort_values(ascending=False)\n",
    "        f1 = pd.DataFrame(f1)\n",
    "        print(f1.head(10))\n",
    "    \n",
    "    def geoSpatialScatterPlot(self):\n",
    "        label=LabelEncoder()\n",
    "        self.dataplot.loc[:,\"Category\"]=label.fit_transform(self.dataplot.loc[:,\"Category\"])\n",
    "        self.dataplot.loc[:,\"DayOfWeek\"]=label.fit_transform(self.dataplot.loc[:,\"DayOfWeek\"])\n",
    "        self.dataplot = self.dataplot[self.dataplot.Y!=90]\n",
    "        Catlabel = pd.DataFrame(list(set(self.train[\"Category\"])))\n",
    "        ax = self.dataplot.plot(kind='scatter', x='X', y='Y', alpha = 0.5, s = self.dataplot[\"DayOfWeek\"], \n",
    "                                label= \"Crime distribution across area\", figsize=(13,9),c='Category', \n",
    "                                cmap=mt.get_cmap('jet'), colorbar=True,)\n",
    "        ax.set_ylabel(\"Latitude\", fontsize=14)\n",
    "        ax.set_xlabel(\"Longitude\", fontsize=14)\n",
    "        mt.show()\n",
    "    \n",
    "    def pdDistrictVsCrime(self):\n",
    "        PdD = list(set(self.dataplot.PdDistrict))\n",
    "        Cat = pd.get_dummies(self.train.Category)\n",
    "        dpd=pd.DataFrame(np.random.normal(0,1,(10,36)),columns=Cat.columns,index=PdD)\n",
    "        for p in PdD:\n",
    "               for ca in Cat.columns:\n",
    "                s = \"PdDistrict == '\"+p+\"' & Category == '\"+ca+\"'\"\n",
    "                d = self.train.query(s)  \n",
    "                dpd.at[p,ca]=d.shape[0]\n",
    "        ax=dpd.plot(kind='bar',rot =0, figsize=(20,13),alpha=0.9,stacked=True)\n",
    "        ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1),ncol=6, fancybox=True, shadow=True)\n",
    "        mt.ylabel(\"Crime Frequency\", fontsize=14)\n",
    "        mt.xlabel(\"PdDistrict\", fontsize=14)\n",
    "        mt.show()\n",
    "        \n",
    "    def hourVsCrime(self):\n",
    "        Hr = list(set(self.dataplot.Hour))\n",
    "        Cat = pd.get_dummies(self.train.Category)\n",
    "        dhr = pd.DataFrame(np.random.normal(0,1,(24,36)),columns=Cat.columns,index=Hr)\n",
    "        for h in Hr:\n",
    "               for ca in Cat.columns:\n",
    "                        s = \"Hour == \"+str(h)+\" & Category == '\"+ca+\"'\"\n",
    "                        d = self.train.query(s)  \n",
    "                        dhr.at[h,ca]=d.shape[0]\n",
    "        ax = dhr.plot( figsize=(23,10),legend=False,alpha=0.9,colormap=cm.cubehelix)\n",
    "        ax.set_ylabel(\"Crime Frequency\")\n",
    "        ax.set_xlabel(\"Hour\")\n",
    "        tick_spacing = 1\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
    "        ax.legend(loc='upper center', bbox_to_anchor=(0.35, 1.05),\n",
    "                  ncol=4, fancybox=True, shadow=True)\n",
    "        mt.show()\n",
    "        \n",
    "    def modifiedHourVsCrime(self):\n",
    "        mh = np.array([28,810,1016,1621,212])\n",
    "        Cat = pd.get_dummies(self.train.Category)\n",
    "        dmh=pd.DataFrame(np.random.normal(0,1,(5,36)),columns=Cat.columns,index=mh)\n",
    "        for h in mh:\n",
    "                for ca in Cat.columns:\n",
    "                        if(h == 28 ): s=\"Hour >=2 & Hour<=8 & Category == '\"+ca+\"'\"\n",
    "                        if(h == 810): s=\"Hour >8 & Hour<=10 & Category == '\"+ca+\"'\"\n",
    "                        if(h == 1016 ): s=\"Hour>10 & Hour<=16 & Category == '\"+ca+\"'\"\n",
    "                        if(h == 1621): s=\"Hour>16 & Hour<=21 & Category == '\"+ca+\"'\"\n",
    "                        if(h== 212): s=\"(Hour>21 or Hour<2) & Category == '\"+ca+\"'\"\n",
    "                        d = self.train.query(s)  \n",
    "                        dmh.at[h,ca]=d.shape[0]\n",
    "                        \n",
    "        ax=dmh.plot(kind='bar',rot =0, figsize=(20,13),alpha=0.9,stacked=True)\n",
    "        ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1),ncol=6, fancybox=True, shadow=True)\n",
    "        mt.ylabel(\"Crime Frequency\", fontsize=14)\n",
    "        mt.xlabel(\"Modified Hour\", fontsize=14)\n",
    "        mt.show() \n",
    "        \n",
    "    def dayofWeekVsCrime(self):\n",
    "        Week = list(set(self.train.DayOfWeek))\n",
    "        Cat = pd.get_dummies(self.train.Category)\n",
    "        dw=pd.DataFrame(np.random.normal(0,1,(7,36)),columns=Cat.columns,index=Week)\n",
    "        for w in Week:\n",
    "               for ca in Cat.columns:\n",
    "                        s = \"DayOfWeek == '\"+w+\"' & Category == '\"+ca+\"'\"\n",
    "                        d = self.train.query(s)  \n",
    "                        dw.at[w,ca]=d.shape[0] \n",
    "        ax=dw.plot(kind='bar',rot =0, figsize=(20,13),alpha=0.9,stacked=True)\n",
    "        ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1),ncol=6, fancybox=True, shadow=True)\n",
    "        mt.ylabel(\"Crime Frequency\", fontsize=14)\n",
    "        mt.xlabel(\"DayOfWeek\", fontsize=14)\n",
    "        mt.show()\n",
    "    \n",
    "    def monthVsCrime(self):\n",
    "        lmon = list(set(self.train.Dates.dt.month))\n",
    "        Cat  = pd.get_dummies(self.train.Category)\n",
    "        dmon = pd.DataFrame(np.random.normal(0,1,(12,36)),columns=Cat.columns,index=lmon)\n",
    "        for p in lmon:\n",
    "               for ca in Cat.columns:\n",
    "                        s = \"Month == \"+str(p)+\" & Category == '\"+ca+\"'\"\n",
    "                        d = self.train.query(s)  \n",
    "                        dmon.at[p,ca]=d.shape[0]\n",
    "        ax = dmon.plot(kind='bar',rot =0, figsize=(20,13),alpha=0.9,stacked=True)\n",
    "        ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1),ncol=6, fancybox=True, shadow=True)\n",
    "        mt.ylabel(\"Crime Frequency\", fontsize=14)\n",
    "        mt.xlabel(\"Month\", fontsize=14)\n",
    "        mt.show() \n",
    "    \n",
    "    def corelationMatrix(self):\n",
    "        tr_hour  = self.train.Dates.dt.hour\n",
    "        tr_year  = self.train.Dates.dt.year\n",
    "        tr_month = self.train.Dates.dt.month\n",
    "        tr_dates = self.train.Dates.dt.day\n",
    "        tr_add           = self.train[\"Address\"]\n",
    "        tr_add_ser       = tr_add.str.contains('.?of.?')\n",
    "        #Label Encoding of Crime Category\n",
    "        cat_le            =  preprocessing.LabelEncoder()\n",
    "        crimecat          =  cat_le.fit_transform(self.train.Category)\n",
    "        \n",
    "        def getseason(mon):\n",
    "            if(mon >= 12 or mon <= 2)   : return 1\n",
    "            elif(mon >= 3 or mon <= 5)  : return 2\n",
    "            elif(mon >= 6 or mon <= 8)  : return 3\n",
    "            elif(mon >= 9 or mon <= 1)  : return 4\n",
    "        tr_month_mapped   = pd.Series(preprocessing.scale(list(map(getseason, tr_month))))\n",
    "        crimecat_ser = pd.Series(crimecat)\n",
    "\n",
    "        pdist_lbe     =  preprocessing.LabelEncoder()\n",
    "        pdist = pdist_lbe.fit_transform(self.train.PdDistrict)\n",
    "        pdist = pd.Series(pdist);\n",
    "        \n",
    "        days_lbe     =  preprocessing.LabelEncoder()\n",
    "        days = days_lbe.fit_transform(self.train.DayOfWeek)\n",
    "        days = pd.Series(days)\n",
    "        \n",
    "        #Applying PCA on X and Y cordinates from training\n",
    "        tr_dataset = self.train.copy()\n",
    "        xy_scaler_tr = preprocessing.StandardScaler()\n",
    "        xy_scaler_tr.fit(tr_dataset[[\"X\",\"Y\"]])\n",
    "        tr_dataset[[\"X\",\"Y\"]] = xy_scaler_tr.transform(tr_dataset[[\"X\",\"Y\"]])\n",
    "        pca_tr = PCA(2)\n",
    "        tr_dataset.loc[:,[\"X\",\"Y\"]]=pca_tr.fit_transform(pd.DataFrame(tr_dataset.loc[:,[\"X\",\"Y\"]]))\n",
    "        tr_dataset[\"rot30_X\"]  = (1.732/2)* tr_dataset[\"X\"] + (1./2)* tr_dataset[\"Y\"] \n",
    "        tr_dataset[\"rot30_Y\"]  = (1.732/2)* tr_dataset[\"Y\"] - (1./2)* tr_dataset[\"X\"]\n",
    "         \n",
    "        tr_dataset[\"rot45_X\"]  = .707* tr_dataset[\"Y\"] + .707* tr_dataset[\"X\"] \n",
    "        tr_dataset[\"rot45_Y\"]  = .707* tr_dataset[\"Y\"] - .707* tr_dataset[\"X\"]\n",
    "         \n",
    "        tr_dataset[\"rot60_X\"]  = (1./2)* tr_dataset[\"X\"] + (1.732/2)* tr_dataset[\"Y\"] \n",
    "        tr_dataset[\"rot60_Y\"]  = (1./2)* tr_dataset[\"Y\"] - (1.732/2)* tr_dataset[\"X\"]\n",
    "        \n",
    "        tr_dataset[\"radial_r\"] = np.sqrt( np.power(tr_dataset[\"Y\"],2) + np.power(tr_dataset[\"X\"],2) )\n",
    "        \n",
    "        abc = pd.concat([crimecat_ser,tr_hour,\n",
    "                         tr_year,tr_month,\n",
    "                          tr_dates,\n",
    "                          tr_add_ser,\n",
    "                          tr_dataset.X,tr_dataset.Y,\n",
    "                          tr_dataset.rot30_X,tr_dataset.rot30_Y,\n",
    "                          tr_dataset.rot45_X,tr_dataset.rot45_Y,\n",
    "                          tr_dataset.rot60_X,tr_dataset.rot60_Y,\n",
    "                          tr_dataset.radial_r,pdist,days,\n",
    "                          tr_month_mapped],axis=1)\n",
    "        lbe     =  preprocessing.LabelEncoder()\n",
    "        corelationmatrix = abc.apply(lbe.fit_transform).corr()\n",
    "        fig, ax = mt.subplots(figsize=(20, 10))\n",
    "        colormap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "        #Generate Heat Map, allow annotations and place floats in map\n",
    "        sns.heatmap(corelationmatrix, cmap=colormap, annot=True, fmt=\".2f\")\n",
    "        mt.show()\n",
    "        \n",
    "    def mostCrimeLoc(self):\n",
    "        top_addresses = self.train.Address.value_counts()[:20]\n",
    "        mt.figure(figsize=(12, 8))\n",
    "\n",
    "        pos = np.arange(len(top_addresses))\n",
    "        mt.barh(pos, top_addresses.values)\n",
    "        mt.yticks(pos, top_addresses.index);\n",
    "    \n",
    "    def crimeAndLocation(self):\n",
    "        top_crimes = self.train.Category.value_counts()[:10]\n",
    "        top_addresses = self.train.Address.value_counts()[:10]\n",
    "        subset = self.train[self.train.Address.isin(top_addresses.index) & self.train.Category.isin(top_crimes.index)]\n",
    "        addr_cross_cat = pd.crosstab(subset.Address, subset.Category)\n",
    "        mt.figure(figsize=(10, 10))\n",
    "        sns.heatmap(addr_cross_cat, linewidths=.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN logloss 26.874481131175887\n",
      "Gaussian Naive Bayes logloss 34.455385273325696\n",
      "Bernoulli Naive Bayes logloss 2.4912385307931935\n",
      "Bernoulli and Gaussian mix Naive Bayes logloss 3.0829764960021624\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    train_data_name = 'train.csv'\n",
    "    test_data_name  = 'test.csv'\n",
    "#=====================Visualisation=================================#    \n",
    "#     visual = SFOVisualisation(train_data_name)\n",
    "#     visual.categoryDistribution()\n",
    "#     visual.geoSpatialScatterPlot()\n",
    "#     visual.pdDistrictVsCrime()\n",
    "#     visual.hourVsCrime()\n",
    "#     visual.modifiedHourVsCrime()\n",
    "#     visual.dayofWeekVsCrime()\n",
    "#     visual.monthVsCrime()\n",
    "#     visual.corelationMatrix()\n",
    "#     visual.mostCrimeLoc()\n",
    "#     visual.crimeAndLocation()\n",
    "#=====================Different Models=================================#    \n",
    "#     model = SFOCrimeClass1(train_data_name,test_data_name)\n",
    "#     model.data()\n",
    "#     model.trainLogisticRegression()\n",
    "#     model.testLogisticRegression()\n",
    "\n",
    "#     model.trainRandomForest()\n",
    "#     model.testRandomForest()\n",
    "    \n",
    "#     model.trainLGBM()\n",
    "#     model.testLGBM()\n",
    "\n",
    "#     model.trainXGB()\n",
    "#     model.testXGB()\n",
    "    model2 = SFOCrimeClass2(train_data_name,test_data_name)\n",
    "    model2.data()\n",
    "    model2.trainKNN()\n",
    "    model2.testKNN()\n",
    "    model2.trainNaiveBayes()\n",
    "    model2.testNaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testUsingPickle(testfile):\n",
    "        te_dataset = pd.read_csv(testFile,parse_dates=['Dates'])\n",
    "     \n",
    "        #Datetime parsering from test data\n",
    "        te_hour  = te_dataset.Dates.dt.hour\n",
    "        te_year  = te_dataset.Dates.dt.year\n",
    "        \n",
    "        #Applying PCA on X and Y cordinates from test\n",
    "        xy_scaler_te = preprocessing.StandardScaler()\n",
    "        xy_scaler_te.fit(te_dataset[[\"X\",\"Y\"]])\n",
    "        te_dataset[[\"X\",\"Y\"]] = xy_scaler_te.transform(te_dataset[[\"X\",\"Y\"]])\n",
    "        pca_te =PCA(2)\n",
    "        te_dataset.loc[:,[\"X\",\"Y\"]] = pca_te.fit_transform(pd.DataFrame(te_dataset.loc[:,[\"X\",\"Y\"]]))\n",
    "        te_dataset[\"rot30_X\"]  = (1.732/2)* te_dataset[\"X\"] + (1./2)* te_dataset[\"Y\"] \n",
    "        te_dataset[\"rot30_Y\"]  = (1.732/2)* te_dataset[\"Y\"] - (1./2)* te_dataset[\"X\"]\n",
    "        te_dataset[\"rot45_X\"]  = .707* te_dataset[\"Y\"] + .707* te_dataset[\"X\"] \n",
    "        te_dataset[\"rot45_Y\"]  = .707* te_dataset[\"Y\"] - .707* te_dataset[\"X\"]\n",
    "        te_dataset[\"rot60_X\"]  = (1./2)* te_dataset[\"X\"] + (1.732/2)* te_dataset[\"Y\"] \n",
    "        te_dataset[\"rot60_Y\"]  = (1./2)* te_dataset[\"Y\"] - (1.732/2)* te_dataset[\"X\"]\n",
    "        te_dataset[\"radial_r\"] = np.sqrt( np.power(te_dataset[\"Y\"],2) + np.power(te_dataset[\"X\"],2) )\n",
    "        #Extracting whther crime happening at juntion or in block\n",
    "        te_add           = te_dataset[\"Address\"]\n",
    "        te_add_ser       = te_add.str.contains('.?of.?')\n",
    "        #Onehot Encoding categorical features\n",
    "        te_hour_ser      = pd.get_dummies(te_hour,prefix = \"H\")\n",
    "        te_year_ser      = pd.get_dummies(te_year)\n",
    "        te_days_ser      = pd.get_dummies(te_dataset.DayOfWeek)\n",
    "        te_pdistrict_ser = pd.get_dummies(te_dataset.PdDistrict)\n",
    "        \n",
    "        testset          = pd.concat([te_hour_ser,\n",
    "                                      te_year_ser,\n",
    "                                      te_days_ser,\n",
    "                                      te_pdistrict_ser,\n",
    "                                      te_add_ser,\n",
    "                                      te_dataset.X,te_dataset.Y,\n",
    "                                      te_dataset.rot30_X,te_dataset.rot30_Y,\n",
    "                                      te_dataset.rot45_X,te_dataset.rot45_Y,\n",
    "                                      te_dataset.rot60_X,te_dataset.rot60_Y,\n",
    "                                      te_dataset.radial_r],axis=1)\n",
    "        \n",
    "        testset_ID = te_dataset['Id']\n",
    "        filename = 'Modelfile.sav'\n",
    "        loaded_model = pickle.load(open(filename, 'rb'))\n",
    "        cat_probabilities = loaded_model.predict_proba(testset)\n",
    "        print(cat_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
