{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2a2692885437b2c0d18e4b4fde003dccec298fd6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing, cross_validation\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors.classification import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn import neighbors\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as mt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "83bf4b91104eb3fd1c7d8268c5cfc9b60ab36e66"
   },
   "outputs": [],
   "source": [
    "\n",
    "    train = pd.read_csv('../input/train.csv', parse_dates=['Dates'])    \n",
    "#     Removing outliers\n",
    "#     train=train[train.Y!=90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e3de2cc7565d385908694f866d09537d24bbcc8c"
   },
   "outputs": [],
   "source": [
    " test = pd.read_csv('../input/test.csv', parse_dates=['Dates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6e39fc520e39a3514584ce1a55b4db4d50a536b0"
   },
   "outputs": [],
   "source": [
    "#     xy_scaler =preprocessing.StandardScaler()\n",
    "#     xy_scaler.fit(train[[\"X\",\"Y\"]])\n",
    "#     train[[\"X\",\"Y\"]]=xy_scaler.transform(train[[\"X\",\"Y\"]])\n",
    "#     pca=PCA(2)\n",
    "#     train.loc[:,[\"X\",\"Y\"]]=pca.fit_transform(pd.DataFrame(train.loc[:,[\"X\",\"Y\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ef82dd6502e267d1896bc14cd0e4cdece026467f"
   },
   "outputs": [],
   "source": [
    "# dll=pd.DataFrame(list(set(train.Category)))\n",
    "# dll\n",
    "# train.head(5)\n",
    "# labels.shape\n",
    "# train.isna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "02efb4e45ac85774167c46d90316d7622f7df624"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "362c866da1bce463ae695c4514911581bda58b85"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9e464fbb9e5bc2dde4f2b5835c28c3b7564c616c"
   },
   "outputs": [],
   "source": [
    "# train.shape\n",
    "# train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "263ec97fd97d92b6eb13fd3d685a71f36bfbbdf8"
   },
   "outputs": [],
   "source": [
    "train.Address.fillna('',inplace=True)\n",
    "df = pd.Series([ x.replace('/','of') for x in train.Address.tolist() ])\n",
    "df=pd.DataFrame([ x.split('of') for x in df.tolist()],columns=[\"Add1\",\"Add2\"])\n",
    "train=pd.concat([train,df[\"Add1\"],df[\"Add2\"]],axis=1)\n",
    "# train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "db9bdb26a8c2c2aa6cd72dc2fb1f0fd20baf72f4"
   },
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = HashingVectorizer(n_features=100)\n",
    "# encode document\n",
    "vector = vectorizer.transform(train.Address)\n",
    "v1=vectorizer.transform(train.Add1)\n",
    "v2=vectorizer.transform(train.Add2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c4e9182a7e44efc50485448979e592f02519acad"
   },
   "outputs": [],
   "source": [
    "svd=TruncatedSVD(n_components=2, random_state=42)\n",
    "train[\"AddSVD\"]=pd.DataFrame(svd.fit_transform(vector)).loc[:,1]\n",
    "train[\"Add1SVD\"]=pd.DataFrame(svd.fit_transform(v1)).loc[:,1]\n",
    "train[\"Add2SVD\"]=pd.DataFrame(svd.fit_transform(v2)).loc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c17201f6dcb7b004d3c00ceb0dc379a6c80d4ce2"
   },
   "outputs": [],
   "source": [
    "train.Descript.fillna('',inplace=True)\n",
    "vector = vectorizer.transform(train.Descript)\n",
    "svd=TruncatedSVD(n_components=2, random_state=42)\n",
    "train[\"DesSVD\"]=pd.DataFrame(svd.fit_transform(vector)).loc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c67ba4251c2b6daa84a62ab726a2452198941844"
   },
   "outputs": [],
   "source": [
    "#     Removing outliers\n",
    "# train=train[train.Y!=90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1999558761c20335ee89e7d1079f1fadd8a87d4b"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=36,max_iter=300)\n",
    "df=pd.DataFrame(pd.concat([pd.DataFrame(train.X),pd.DataFrame(train.Y)],axis=1))\n",
    "kmeans.fit(df)\n",
    "labels = kmeans.predict(df)\n",
    "centroids = kmeans.cluster_centers_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8fa4c7e951c0e8fb454eec0985b6c361869b11a5"
   },
   "outputs": [],
   "source": [
    "# fig = mt.figure(figsize=(5, 5))\n",
    "\n",
    "# # colors = map(lambda x: colmap[x+1], labels)\n",
    "\n",
    "# mt.scatter(df['X'], df['Y'], alpha=0.5,cmap=mt.get_cmap('jet'), edgecolor='k')\n",
    "# for idx, centroid in enumerate(centroids):\n",
    "# #     mt.scatter(*centroid, color=colmap[idx+1])\n",
    "#     mt.scatter(*centroid)\n",
    "# # mt.xlim(0, 80)\n",
    "# # mt.ylim(0, 80)\n",
    "# mt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f4ff0f530ac942851882aed44d0ad1b403a459a0"
   },
   "outputs": [],
   "source": [
    "le_crime = preprocessing.LabelEncoder()\n",
    "crime = le_crime.fit_transform(train.Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c012993aeff42c4df722a4400153ef0ba106130d"
   },
   "outputs": [],
   "source": [
    "# def divmon(m):\n",
    "#     if (m>=1 and m<=2): return 'A'\n",
    "#     if (m>=3 and m<=5): return 'B'\n",
    "#     if (m>=5 and m<=9): return 'C'\n",
    "#     if (m==10): return 'D'\n",
    "#     if (m>=11 and m<=12): return 'E'\n",
    "mon = train.Dates.dt.month\n",
    "# mon = pd.DataFrame([divmon(m) for m in mon.tolist()])\n",
    "mon=pd.get_dummies(mon,prefix='m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "51a447048b320008f9fa0bfd4816702aaaf3d57d"
   },
   "outputs": [],
   "source": [
    "\n",
    "# crime = train.Category\n",
    "crimelist = pd.get_dummies(train.Category) \n",
    "days = pd.get_dummies(train.DayOfWeek)\n",
    "district = pd.get_dummies(train.PdDistrict)\n",
    "# hour = train.Dates.dt.hour\n",
    "hour = pd.get_dummies(train.Dates.dt.hour,prefix='h')\n",
    "minu = train.Dates.dt.minute\n",
    "# Klabels=pd.get_dummies(labels,prefix='kl')\n",
    "# train[\"hour\"] = train[\"hour\"]*60+minu\n",
    "# address = pd.get_dummies(train.Address)\n",
    "# desc = pd.get_dummies(train.Descript)\n",
    "minu = pd.get_dummies(minu)\n",
    "\n",
    "train[\"Klabels\"]=labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a2ed787d111ac3adace0206e6772ab38267f5b99"
   },
   "outputs": [],
   "source": [
    "    train_data = pd.concat([minu,mon, train.Klabels,days,hour,district,train.X,train.Y,train.AddSVD,train.Add1SVD,train.Add2SVD,train.DesSVD], axis=1)\n",
    "    train_data['crime'] = crime\n",
    "    train_data = pd.concat([train_data,crimelist],axis=1)\n",
    "#     train_data = pd.concat([train_data,Klabels],axis=1)\n",
    "#     train_data['hour'] = hour\n",
    "#     train_data['mon'] = mon\n",
    "\n",
    "#     train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "01ae5315db4a9b17d97733dfbc5050746729bdab",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_data.fillna(0,inplace=True)\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8e374093a2876eb9c15995d7adbb59e26bdeef27"
   },
   "outputs": [],
   "source": [
    "# train_data1 = pd.concat([train_data,pd.DataFrame(Klabels)],axis=1)\n",
    "# train_data1=train_data.join(Klabels)\n",
    "# for i in range(len(Klabels.columns)):\n",
    "#     train_data1 = train_data.assign(Klabels.loc[:,i])\n",
    "# train_data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "76994eab04b7d817f109241b73769eb09180cf0b"
   },
   "outputs": [],
   "source": [
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "31f3c0595fd9505f1b89fb603ff537ea62199806"
   },
   "outputs": [],
   "source": [
    "    days = pd.get_dummies(test.DayOfWeek)\n",
    "    district = pd.get_dummies(test.PdDistrict)\n",
    "    minu = test.Dates.dt.minute\n",
    "    minu = pd.get_dummies(minu)\n",
    "    hour = pd.get_dummies(test.Dates.dt.hour,prefix='h')\n",
    "#     hour = test.Dates.dt.hour\n",
    "#     test[\"hour\"] = test[\"hour\"]*60+minu\n",
    "\n",
    "#     address = le_crime.fit_transform(test.Address)\n",
    "#     desc = le_crime.fit_transform(test.Descript)\n",
    "    \n",
    "    mon = test.Dates.dt.month\n",
    "#     mon = pd.DataFrame([divmon(m) for m in mon.tolist()])\n",
    "\n",
    "    mon=pd.get_dummies(mon,prefix='m')\n",
    "    \n",
    "#     test[[\"X\",\"Y\"]]=xy_scaler.fit_transform(test[[\"X\",\"Y\"]])\n",
    "#     pca=PCA(2)\n",
    "#     test.loc[:,[\"X\",\"Y\"]]=pca.fit_transform(pd.DataFrame(test.loc[:,[\"X\",\"Y\"]]))\n",
    "    \n",
    "    d = pd.Series([ x.replace('/','of') for x in test.Address.tolist() ])\n",
    "    d=pd.DataFrame([ x.split('of') for x in d.tolist()],columns=[\"Add1\",\"Add2\"])\n",
    "    test=pd.concat([test,d[\"Add1\"],d[\"Add2\"]],axis=1)\n",
    "    \n",
    "    vector = vectorizer.transform(test.Address)\n",
    "    v1=vectorizer.transform(test.Add1)\n",
    "    v2=vectorizer.transform(test.Add2)\n",
    "    \n",
    "    svd=TruncatedSVD(n_components=10, random_state=42)\n",
    "    test[\"AddSVD\"]=pd.DataFrame(svd.fit_transform(vector)).loc[:,1]\n",
    "    test[\"Add1SVD\"]=pd.DataFrame(svd.fit_transform(v1)).loc[:,1]\n",
    "    test[\"Add2SVD\"]=pd.DataFrame(svd.fit_transform(v2)).loc[:,1]\n",
    "    \n",
    "    vector = vectorizer.transform(test.Descript)\n",
    "    test[\"DesSVD\"]=pd.DataFrame(svd.fit_transform(vector)).loc[:,1]\n",
    "\n",
    "    df=pd.DataFrame(pd.concat([pd.DataFrame(test.X),pd.DataFrame(test.Y)],axis=1))\n",
    "#     kmeans = KMeans(n_clusters=200,max_iter=300)\n",
    "    kmeans.fit(df)\n",
    "    labels = kmeans.predict(df)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    test[\"Klabels\"]=labels\n",
    "#     Klabels=pd.get_dummies(labels,prefix='kl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1ff51b99f7f08eb6c308f11589e8343164ac81ee"
   },
   "outputs": [],
   "source": [
    "    test_data = pd.concat([test.Id,test.Klabels,minu,mon,hour,days,district,test.X,test.Y,test.AddSVD,test.Add1SVD,test.Add2SVD,test.DesSVD], axis=1)\n",
    "#     test_data = pd.concat([test_data,Klabels],axis=1)\n",
    "#     test_data['hour'] = hour\n",
    "# #     test_data['mon'] = mon\n",
    "# #     test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4021218436598422e394c823438a85bb5fcb1bc5"
   },
   "outputs": [],
   "source": [
    "features1 = ['Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday','Wednesday', \n",
    "            'BAYVIEW', 'CENTRAL', 'INGLESIDE', 'MISSION','NORTHERN', 'PARK', 'RICHMOND',\n",
    "             'SOUTHERN', 'TARAVAL','TENDERLOIN'] + [x for x in range(0, 59)]+ [x for x in (set(pd.DataFrame(hour).columns))]\n",
    "# features1 = features1 + [x for x in (set(pd.DataFrame(Klabels).columns))]\n",
    "# +[x for x in (set(pd.DataFrame(mon).columns))]\n",
    "#             ,'Add2SVD'\n",
    "#     ,'hour'\n",
    "features2 = ['AddSVD','Klabels','X','Y','DesSVD']\n",
    "features = features1+features2\n",
    "features3 = ['hour','minu','district','Y','DesSVD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f485f2ad761d71c2f03c7e1e689c09d272ca7916"
   },
   "outputs": [],
   "source": [
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0dea24f9ed5777e844161df8159b45e5eb825b8d"
   },
   "outputs": [],
   "source": [
    "# le_crime = preprocessing.LabelEncoder()\n",
    "#     crime = le_crime.fit_transform(train.Category)\n",
    "# crime = train.Category\n",
    "# crimelist = pd.get_dummies(train.Category) \n",
    "lday=preprocessing.LabelEncoder()\n",
    "ldistrict=preprocessing.LabelEncoder()\n",
    "# days = pd.get_dummies(train.DayOfWeek)\n",
    "train[\"days\"] = lday.fit_transform(train.DayOfWeek)\n",
    "# district = pd.get_dummies(train.PdDistrict)\n",
    "train[\"district\"] = ldistrict.fit_transform(train.PdDistrict)\n",
    "train[\"hour\"] = train.Dates.dt.hour\n",
    "train[\"minu\"] = train.Dates.dt.minute\n",
    "\n",
    "train_data1 = pd.concat([crimelist,train.hour,train.minu,train.days,train.district,train.X,train.Y,train.DesSVD], axis=1)\n",
    "\n",
    "test[\"days\"] = lday.fit_transform(test.DayOfWeek)\n",
    "# district = pd.get_dummies(train.PdDistrict)\n",
    "test[\"district\"] = ldistrict.transform(test.PdDistrict)\n",
    "test[\"hour\"] = test.Dates.dt.hour\n",
    "test[\"minu\"] = test.Dates.dt.minute\n",
    "\n",
    "test_data1 = pd.concat([test.Id,test.hour,test.minu,test.days,test.district,test.X,test.Y,test.DesSVD], axis=1)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "29cabd312180a6352ebf08cb40fa1bba3456f5ba"
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "ll= pd.DataFrame(np.random.normal(0,1,(5,6)),columns=[\"Training Size\",\"Log LossBNB\",\"Log LossGNB\",\"StackedLR\",\"KNN\",\"Log LossMIX\"])\n",
    "# ll= pd.DataFrame(np.random.normal(0,1,(16,2)),columns=[\"Neighbours\",\"Log Loss\"])\n",
    "\n",
    "train_data[features].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3a9e2b15612daa8bc18c3826fee5346d96bac2e0"
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "train_data = shuffle(train_data)\n",
    "sampletest,remtrain = train_test_split(train_data, test_size=(0.2))\n",
    "for j in range(70,95,20):\n",
    "    training, validation = train_test_split(remtrain, train_size=(j/100))\n",
    "#     training = train_data\n",
    "#     validation = test_data\n",
    "#     train_data1 = shuffle(train_data1)\n",
    "#     training1, validation1 = train_test_split(train_data1, train_size=(j/100))\n",
    "    \n",
    "    ll.at[count,[\"Training Size\"]]=str(j)\n",
    "    model = GaussianNB()\n",
    "    model.fit(training[features2], training['crime'])\n",
    "    predicted = np.array(model.predict_proba(validation[features2]))\n",
    "    ll.at[count,[\"Log LossGNB\"]]=str(log_loss(validation['crime'], predicted))\n",
    "    modelB = BernoulliNB()\n",
    "    modelB.fit(training[features], training['crime'])\n",
    "    predictedB = np.array(modelB.predict_proba(validation[features]))\n",
    "    ll.at[count,[\"Log LossBNB\"]]=str(log_loss(validation['crime'], predictedB))\n",
    "    \n",
    "#     Logistic Regression:- Stacking with bernoulli \n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(modelB.predict(validation[features]).reshape(-1,1),validation['crime'])\n",
    "#     modelB.fit(remtrain[features], remtrain['crime'])\n",
    "#     predictedB = np.array(modelB.predict_proba(sampletest[features]))\n",
    "    predictedSLR=logreg.predict_proba(modelB.predict(sampletest[features]).reshape(-1,1))\n",
    "    ll.at[count,[\"StackedLR\"]]=str(log_loss(sampletest['crime'], predictedSLR))\n",
    "    \n",
    "#     modelMNB = MultinomialNB()\n",
    "    modelBNB = BernoulliNB()\n",
    "    modelGNB = GaussianNB()\n",
    "    modelBNB.fit(training[features1], training['crime'])\n",
    "    modelGNB.fit(training[features2], training['crime'])\n",
    "#     modelMNB.fit(training[features5], training['crime'])\n",
    "#     calculating the log loss function with the validation section and printing the value\n",
    "# class_prior Prior probabilities of the classes. If specified the priors are not adjusted according to the data.\n",
    "    pos_prior = modelGNB.class_prior_\n",
    "    predictedBNB = np.array(modelBNB.predict_proba(validation[features1]))\n",
    "    predictedGNB = np.array(modelGNB.predict_proba(validation[features2]))\n",
    "#     predictedMNB = np.array(modelMNB.predict_proba(validation[features5]))\n",
    "#     predicted = (predictedBNB)* (predictedGNB/pos_prior)\n",
    "    predicted = (predictedBNB)* (predictedGNB)\n",
    "    ll.at[count,[\"Log LossMIX\"]]=str(log_loss(validation['crime'], predicted))\n",
    "    \n",
    "   \n",
    "#     model = MultinomialNB()\n",
    "#     model.fit(training1[features3], training1['crime'])\n",
    "#     predictedm = np.array(model.predict_proba(validation1[features3]))\n",
    "#     ll.at[count,[\"Log LossMNB\"]]=str(log_loss(validation1['crime'], predictedm))\n",
    "    \n",
    "    # KNN\n",
    "    train_data1 = shuffle(train_data1)\n",
    "    training1, validation1 = train_test_split(train_data1, train_size=.80)\n",
    "    n_neighbors=1300\n",
    "    # for n_neighbors in range(1000,2010,500):\n",
    "    # for i, weights in enumerate(['uniform', 'distance']):\n",
    "    knn = neighbors.KNeighborsRegressor(n_neighbors,n_jobs=500,weights='distance')\n",
    "    knn.fit(training1[features3],training1[list(pd.DataFrame(crimelist).columns)])\n",
    "    predicted = knn.predict(validation1[features3])\n",
    "    ll.at[count,[\"KNN\"]]=str(log_loss(validation1[list(pd.DataFrame(crimelist).columns)], predicted))\n",
    "    \n",
    "    \n",
    "    count=count+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ca5c969e1f08f8270388dd47c534af52d1e9f2f"
   },
   "outputs": [],
   "source": [
    "ll\n",
    "# (pd.DataFrame(sampletest['crime'])).values\n",
    "# (pd.DataFrame(modelB.predict(validation[features])).values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "81b2a6ed73f56f56bac4a7142ebca872da66b767"
   },
   "outputs": [],
   "source": [
    "# # save the model to disk\n",
    "# filename = 'BernoulliNB_all_fN.sav'\n",
    "# pickle.dump(modelB, open(filename, 'wb'))\n",
    "# filename = 'BNBN.sav'\n",
    "# pickle.dump(modelBNB, open(filename, 'wb'))\n",
    "# filename = 'GNBN.sav'\n",
    "# pickle.dump(modelGNB, open(filename, 'wb'))\n",
    "# filename = 'KNN.sav'\n",
    "# pickle.dump(modelB, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0cb87dd1da339166b395c780682a5e2a21be9f05"
   },
   "outputs": [],
   "source": [
    "# # KNN\n",
    "# # train_data = shuffle(train_data)\n",
    "# # training, validation = train_test_split(train_data, train_size=.80)\n",
    "# n_neighbors=1500\n",
    "# # for n_neighbors in range(2000,2010,1000):\n",
    "# # for i, weights in enumerate(['uniform', 'distance']):\n",
    "# knn = neighbors.KNeighborsRegressor(n_neighbors,n_jobs=500)\n",
    "# knn.fit(training[features],training['crime'])\n",
    "# predicted = knn.predict(validation[features])\n",
    "# ll.at[count,[\"Neighbours\"]]=str(n_neighbors)\n",
    "# ll.at[count,[\"Log Loss\"]]=str(log_loss(validation[list(pd.DataFrame(crime).columns)], predicted))\n",
    "# count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fbb67f73af9032d579ac631acedbae6f9bbac26a"
   },
   "outputs": [],
   "source": [
    "# crime = le_crime.transform(train.Category)\n",
    "# pd.DataFrame(predicted).head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b351d325eec06a9f4e1823468228366e5ad9d466"
   },
   "outputs": [],
   "source": [
    "# ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cc9ae98210a4d76f158973fbe8b3bd21c037a68a"
   },
   "outputs": [],
   "source": [
    "# model = BernoulliNB()\n",
    "# model.fit(train_data[features], train_data['crime'])\n",
    "# predicted = model.predict_proba(test_data[features])\n",
    "# # predicted=pd.DataFrame(predicted, columns = le_crime.classes_)\n",
    "# # result=pd.concat([predicted,test_data['Id']],axis=1)\n",
    "# # # result.to_csv('~/Desktop/San Files/submitNB.csv')\n",
    "# # modelBNB = BernoulliNB()\n",
    "# # modelGNB = GaussianNB()\n",
    "# # modelBNB.fit(train_data[features1], train_data['crime'])\n",
    "# # modelGNB.fit(train_data[features2], train_data['crime'])\n",
    "# # pos_prior = modelGNB.class_prior_\n",
    "# # predictedBNB = np.array(modelBNB.predict_proba(test_data[features1]))\n",
    "# # predictedGNB = np.array(modelGNB.predict_proba(test_data[features2]))\n",
    "# # predicted = (predictedBNB)* ( predictedGNB/pos_prior)\n",
    "# predicted = knn.predict(test_data1[features3])\n",
    "# predicted = pd.DataFrame(predicted, columns = list(pd.DataFrame(crimelist).columns))\n",
    "# # predicted=pd.DataFrame(predicted, columns = le_crime.classes_)\n",
    "# # pd.DataFrame(predicted).head(5)\n",
    "# result=pd.concat([predicted,test_data1['Id']],axis=1)\n",
    "# result.to_csv('submitKNN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b52549f7ef68e9b483143915b6d9db2752ded93b"
   },
   "outputs": [],
   "source": [
    "# KNN \n",
    "# n_neighbors=10\n",
    "# knn = neighbors.KNeighborsRegressor(n_neighbors)\n",
    "# knn.fit(train_data[features],train_data[list(pd.DataFrame(crime).columns)])\n",
    "# predicted = knn.predict(test_data[features])\n",
    "# predicted = pd.DataFrame(predicted, columns = list(pd.DataFrame(crime).columns))\n",
    "# result=pd.concat([predicted,test_data['Id']],axis=1)\n",
    "# result.to_csv('~/Desktop/San Files/submitNB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8e9c4043d6f92a6f3080b1a2bf40ef2e378e218b"
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ecbc0de8ac147e4f21affea7f794e9c30ae73049"
   },
   "outputs": [],
   "source": [
    "#     predicted_val = model.predict(test_data[features])\n",
    "#     predicted_val = le_crime.inverse_transform(predicted_val)\n",
    "#     result = pd.DataFrame(predicted_val)\n",
    "#     result.to_csv('~/Desktop/San Files/resultsNB.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5c3745b479a957faa0df5075b15266eaf1458391"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
